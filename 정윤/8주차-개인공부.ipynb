{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지난시간에는...\n",
    "\n",
    "### 검증세트\n",
    "원래는 훈련세트 80%, 테스트 세트 20%로 나누어 테스트 세트를 통해 하이퍼파라메타를 결정해주었다.  \n",
    "하지만 이렇게 하면 테스트 세트에 오버피팅이 될 수도 있기 때문에 그것에 대한 해결방법으로  \n",
    "훈련세트 80%를 60%, 20%로 나누어, 훈련세트와 \"검증세트\"를 추가로 만들었었다.  \n",
    "학습을 시키게 될 때, 훈련+검증 세트로 훈련을 시키고, score를 내게 될 때 검증 세트도 같이 score를 내서  \n",
    "테스트 세트에 오버피팅 되지 않도록 만들어준다.\n",
    "\n",
    "### 교차 검증\n",
    "훈련세트, 검증세트를 데이터 안에서 K번 섞어서(?) 모델을 훈련하고 모델을 평가함으로써 오버피팅을 막는 방법이다.  \n",
    "K번 섞는다고 해서 ***K-폴드 교차검증*** 이라고 한다.  \n",
    "사이킷런에서는 5폴드가 기본으로 제공된다.  \n",
    "함수 이름은 `cross_validate(model, train_input, train_target)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이번 시간에는...\n",
    "\n",
    "### 머신러닝\n",
    "- 지도학습(supervised learning) - feature(X) + class(Y)\n",
    "  - 회귀(예측) - Y값이 연속적\n",
    "  - 분류 - Y값이 이산적\n",
    "  - **지도학습의 한계** : Y값을 누군가가 정해주어야하기 때문에 인력이 필요하다. = 비용이 발생한다.\n",
    "- 비지도학습(self-supervised learning) - feature(X)만 있는 것\n",
    "  - 클러스터링(군집화) : 유사한 것 끼리 모아준다.\n",
    "- 강화학습(reinforced learning) - X값 마저 기계가 찾는다\n",
    "  - 상벌제도(Reward)\n",
    "\n",
    "### 정형 vs 비정형 데이터\n",
    "정형 데이터(Structured)는 숫자로 나타나는 데이터고,  \n",
    "비정형 데이터(Unstructured)는 숫자로 나타나지 않는 데이터다.(텍스트, 오디오, 이미지, 영상...)  \n",
    "보통은 정형 데이터는 머신러닝으로 처리하고, 비정형 데이터는 딥러닝으로 처리한다.\n",
    "\n",
    "## 트리 앙상블 알고리즘\n",
    "지금까지 배운 머신러닝 알고리즘 중에 가장 성능이 뛰어난 알고리즘  \n",
    "대표적인 트리 앙상블 알고리즘은 ***랜덤 포레스트***  \n",
    "코드로는 `RandomForestClassifier` & `RandomForestRegressor`  \n",
    "결정 트리를 랜덤하게 만들어서 숲을 만드는 알고리즘이다.\n",
    "\n",
    "### 랜덤 포레스트의 훈련 방법\n",
    "#### 부트스트랩 샘플\n",
    "랜덤 포레스트는 ***부트스트랩 샘플*** 이라는 것을 뽑아낸다.  \n",
    "**부트스트랩 샘플 :** 중복을 허용하여 원래 데이터 크기 만큼 랜덤하게 뽑아낸다.  \n",
    "\n",
    ">예) **동그라미1, 네모2, 세모2** 이렇게 있는데  \n",
    "부트스트랩 샘플을 뽑아내면  \n",
    "**세모3, 동그라미2** 이렇게 뽑아낼 수 있다는 말\n",
    "\n",
    "이렇게 하는 이유는 결정트리의 성능이 너무 좋아 오버피팅되기 때문에  \n",
    "랜덤으로 값을 뽑아내서 고의적으로 성능을 끌어내리는 것이다.\n",
    "\n",
    "이렇게 만들어진 트리를 여러개 만들어서 일반화 하면 랜덤 포레스트가 만들어진다.  \n",
    "(기본적으로 100개의 트리 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 랜덤포레스트\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
    "\n",
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()\n",
    "\n",
    "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8905151032797809\n",
      "[0.23167441 0.50039841 0.26792718]\n",
      "0.8934000384837406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 이름은 테스트 스코어지만 train만 넣어주었기 때문에 사실 검증세트다.\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "rf.fit(train_input, train_target)\n",
    "# 데이터의 중요도 출력\n",
    "print(rf.feature_importances_)\n",
    "\n",
    "# oob : 부트스트랩 샘플을 뽑아낼 때 뽑아내지 않은, 트리를 만들지 않은 샘플들\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "\n",
    "rf.fit(train_input, train_target)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엑스트라트리\n",
    "랜덤포레스트와 비슷한데 부트스트랩 샘플을 사용하지 않음  \n",
    "무작위성을 다른 곳에서 찾은 알고리즘 이라고 생각하면 댐  \n",
    "노트 분할을 무작위로 한다(추가 설명이 필요할 것 같다...)  \n",
    "가장 좋은 불순도 차이를 나타내는 것을 찾아냄  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n",
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "## 엑스트라트리\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "et.fit(train_input, train_target)\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그레이디언트 부스팅\n",
    "트리 앙상블 중에 가장 좋은 알고리즘  \n",
    "분류할 때는 로지스틱 함수를 썼고, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n",
      "0.9464595437171814 0.8780082549788999\n",
      "[0.15872278 0.68010884 0.16116839]\n",
      "0.9321723946453317 0.8801241948619236\n",
      "[0.08876275 0.23438522 0.08027708]\n",
      "[0.05969231 0.20238462 0.049     ]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/solstice/Desktop/Github/Inhatc-MachineLearning/정윤/8주차-개인공부.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solstice/Desktop/Github/Inhatc-MachineLearning/%EC%A0%95%EC%9C%A4/8%EC%A3%BC%EC%B0%A8-%EA%B0%9C%EC%9D%B8%EA%B3%B5%EB%B6%80.ipynb#ch0000008?line=39'>40</a>\u001b[0m hgb\u001b[39m.\u001b[39mscore(test_input, test_target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solstice/Desktop/Github/Inhatc-MachineLearning/%EC%A0%95%EC%9C%A4/8%EC%A3%BC%EC%B0%A8-%EA%B0%9C%EC%9D%B8%EA%B3%B5%EB%B6%80.ipynb#ch0000008?line=41'>42</a>\u001b[0m \u001b[39m#### XGBoost\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/solstice/Desktop/Github/Inhatc-MachineLearning/%EC%A0%95%EC%9C%A4/8%EC%A3%BC%EC%B0%A8-%EA%B0%9C%EC%9D%B8%EA%B3%B5%EB%B6%80.ipynb#ch0000008?line=43'>44</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxgboost\u001b[39;00m \u001b[39mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solstice/Desktop/Github/Inhatc-MachineLearning/%EC%A0%95%EC%9C%A4/8%EC%A3%BC%EC%B0%A8-%EA%B0%9C%EC%9D%B8%EA%B3%B5%EB%B6%80.ipynb#ch0000008?line=45'>46</a>\u001b[0m xgb \u001b[39m=\u001b[39m XGBClassifier(tree_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhist\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/solstice/Desktop/Github/Inhatc-MachineLearning/%EC%A0%95%EC%9C%A4/8%EC%A3%BC%EC%B0%A8-%EA%B0%9C%EC%9D%B8%EA%B3%B5%EB%B6%80.ipynb#ch0000008?line=46'>47</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_validate(xgb, train_input, train_target, return_train_score\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "## 그레이디언트 부스팅\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "gb.fit(train_input, train_target)\n",
    "print(gb.feature_importances_)\n",
    "\n",
    "## 히스토그램 기반 부스팅\n",
    "\n",
    "# 사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.\n",
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "hgb.fit(train_input, train_target)\n",
    "result = permutation_importance(hgb, train_input, train_target, n_repeats=10,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "print(result.importances_mean)\n",
    "\n",
    "result = permutation_importance(hgb, test_input, test_target, n_repeats=10,\n",
    "                                random_state=42, n_jobs=-1)\n",
    "print(result.importances_mean)\n",
    "\n",
    "hgb.score(test_input, test_target)\n",
    "\n",
    "#### XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "#### LightGBM\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa3dd84ef0f650d9d8b867844db5a915f08cb58b5d6dab20fc53865ee0283ae4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
