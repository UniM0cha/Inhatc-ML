{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14장 모델의 성능 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://raw.githubusercontent.com/taehojo/taehojo.github.io/master/assets/images/linktocolab.png\" align=\"left\"/> ](https://colab.research.google.com/github/taehojo/deeplearning/blob/master/colab/ch14-colab.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터의 확인과 검증셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 데이터를 미리 보겠습니다.\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_117 (Dense)           (None, 30)                390       \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 12)                372       \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 6.5580 - accuracy: 0.2528 - val_loss: 3.5861 - val_accuracy: 0.2285\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.6852 - accuracy: 0.4137 - val_loss: 0.3179 - val_accuracy: 0.8631\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3758 - accuracy: 0.8306 - val_loss: 0.3586 - val_accuracy: 0.8538\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8389 - val_loss: 0.3268 - val_accuracy: 0.8685\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3323 - accuracy: 0.8640 - val_loss: 0.2607 - val_accuracy: 0.8977\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2699 - accuracy: 0.8922 - val_loss: 0.2514 - val_accuracy: 0.8962\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2615 - accuracy: 0.9043 - val_loss: 0.2350 - val_accuracy: 0.9069\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9099 - val_loss: 0.2172 - val_accuracy: 0.9185\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2324 - accuracy: 0.9161 - val_loss: 0.2102 - val_accuracy: 0.9269\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2253 - accuracy: 0.9194 - val_loss: 0.2084 - val_accuracy: 0.9254\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2225 - accuracy: 0.9212 - val_loss: 0.2056 - val_accuracy: 0.9269\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9246 - val_loss: 0.2005 - val_accuracy: 0.9277\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.9243 - val_loss: 0.1956 - val_accuracy: 0.9285\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9264 - val_loss: 0.1929 - val_accuracy: 0.9292\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2110 - accuracy: 0.9258 - val_loss: 0.1910 - val_accuracy: 0.9300\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2088 - accuracy: 0.9276 - val_loss: 0.1887 - val_accuracy: 0.9308\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2072 - accuracy: 0.9289 - val_loss: 0.1868 - val_accuracy: 0.9346\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2058 - accuracy: 0.9289 - val_loss: 0.1856 - val_accuracy: 0.9377\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2047 - accuracy: 0.9281 - val_loss: 0.1847 - val_accuracy: 0.9377\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9297 - val_loss: 0.1828 - val_accuracy: 0.9392\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2028 - accuracy: 0.9299 - val_loss: 0.1818 - val_accuracy: 0.9385\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.9292 - val_loss: 0.1818 - val_accuracy: 0.9408\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2011 - accuracy: 0.9294 - val_loss: 0.1800 - val_accuracy: 0.9385\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2006 - accuracy: 0.9307 - val_loss: 0.1793 - val_accuracy: 0.9385\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9302 - val_loss: 0.1786 - val_accuracy: 0.9423\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1992 - accuracy: 0.9307 - val_loss: 0.1773 - val_accuracy: 0.9392\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9307 - val_loss: 0.1769 - val_accuracy: 0.9438\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9307 - val_loss: 0.1756 - val_accuracy: 0.9431\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1971 - accuracy: 0.9312 - val_loss: 0.1747 - val_accuracy: 0.9408\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9305 - val_loss: 0.1749 - val_accuracy: 0.9438\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1954 - accuracy: 0.9310 - val_loss: 0.1730 - val_accuracy: 0.9423\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1942 - accuracy: 0.9320 - val_loss: 0.1724 - val_accuracy: 0.9438\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1933 - accuracy: 0.9312 - val_loss: 0.1717 - val_accuracy: 0.9454\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1924 - accuracy: 0.9312 - val_loss: 0.1702 - val_accuracy: 0.9462\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1915 - accuracy: 0.9328 - val_loss: 0.1696 - val_accuracy: 0.9462\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1907 - accuracy: 0.9335 - val_loss: 0.1686 - val_accuracy: 0.9462\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1901 - accuracy: 0.9330 - val_loss: 0.1680 - val_accuracy: 0.9462\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.9328 - val_loss: 0.1675 - val_accuracy: 0.9446\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.9351 - val_loss: 0.1666 - val_accuracy: 0.9469\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1876 - accuracy: 0.9346 - val_loss: 0.1659 - val_accuracy: 0.9454\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1867 - accuracy: 0.9364 - val_loss: 0.1659 - val_accuracy: 0.9462\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9343 - val_loss: 0.1656 - val_accuracy: 0.9454\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.9369 - val_loss: 0.1636 - val_accuracy: 0.9462\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9353 - val_loss: 0.1659 - val_accuracy: 0.9469\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1834 - accuracy: 0.9369 - val_loss: 0.1628 - val_accuracy: 0.9462\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1822 - accuracy: 0.9371 - val_loss: 0.1645 - val_accuracy: 0.9477\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1808 - accuracy: 0.9369 - val_loss: 0.1631 - val_accuracy: 0.9492\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1797 - accuracy: 0.9369 - val_loss: 0.1616 - val_accuracy: 0.9492\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1780 - accuracy: 0.9376 - val_loss: 0.1613 - val_accuracy: 0.9492\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9379 - val_loss: 0.1589 - val_accuracy: 0.9492\n"
     ]
    }
   ],
   "source": [
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델을 실행합니다.\n",
    "# 80대 20으로 나눠진 것에서 80% 쪽에서 25% 또 떼어내서 검증셋 설정\n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25) # 0.8 x 0.25 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 802us/step - loss: 0.1554 - accuracy: 0.9531\n",
      "Test accuracy: 0.9530768990516663\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 업데이트하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_121 (Dense)           (None, 30)                390       \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 12)                372       \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델의 저장 설정 및 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./data/model/all/01-0.7538.hdf5\n",
      "\n",
      "Epoch 2: saving model to ./data/model/all/02-0.7538.hdf5\n",
      "\n",
      "Epoch 3: saving model to ./data/model/all/03-0.7454.hdf5\n",
      "\n",
      "Epoch 4: saving model to ./data/model/all/04-0.7623.hdf5\n",
      "\n",
      "Epoch 5: saving model to ./data/model/all/05-0.8000.hdf5\n",
      "\n",
      "Epoch 6: saving model to ./data/model/all/06-0.8477.hdf5\n",
      "\n",
      "Epoch 7: saving model to ./data/model/all/07-0.8677.hdf5\n",
      "\n",
      "Epoch 8: saving model to ./data/model/all/08-0.8908.hdf5\n",
      "\n",
      "Epoch 9: saving model to ./data/model/all/09-0.9023.hdf5\n",
      "\n",
      "Epoch 10: saving model to ./data/model/all/10-0.9169.hdf5\n",
      "\n",
      "Epoch 11: saving model to ./data/model/all/11-0.9262.hdf5\n",
      "\n",
      "Epoch 12: saving model to ./data/model/all/12-0.9300.hdf5\n",
      "\n",
      "Epoch 13: saving model to ./data/model/all/13-0.9323.hdf5\n",
      "\n",
      "Epoch 14: saving model to ./data/model/all/14-0.9315.hdf5\n",
      "\n",
      "Epoch 15: saving model to ./data/model/all/15-0.9315.hdf5\n",
      "\n",
      "Epoch 16: saving model to ./data/model/all/16-0.9315.hdf5\n",
      "\n",
      "Epoch 17: saving model to ./data/model/all/17-0.9346.hdf5\n",
      "\n",
      "Epoch 18: saving model to ./data/model/all/18-0.9385.hdf5\n",
      "\n",
      "Epoch 19: saving model to ./data/model/all/19-0.9377.hdf5\n",
      "\n",
      "Epoch 20: saving model to ./data/model/all/20-0.9408.hdf5\n",
      "\n",
      "Epoch 21: saving model to ./data/model/all/21-0.9400.hdf5\n",
      "\n",
      "Epoch 22: saving model to ./data/model/all/22-0.9408.hdf5\n",
      "\n",
      "Epoch 23: saving model to ./data/model/all/23-0.9423.hdf5\n",
      "\n",
      "Epoch 24: saving model to ./data/model/all/24-0.9423.hdf5\n",
      "\n",
      "Epoch 25: saving model to ./data/model/all/25-0.9438.hdf5\n",
      "\n",
      "Epoch 26: saving model to ./data/model/all/26-0.9446.hdf5\n",
      "\n",
      "Epoch 27: saving model to ./data/model/all/27-0.9454.hdf5\n",
      "\n",
      "Epoch 28: saving model to ./data/model/all/28-0.9492.hdf5\n",
      "\n",
      "Epoch 29: saving model to ./data/model/all/29-0.9431.hdf5\n",
      "\n",
      "Epoch 30: saving model to ./data/model/all/30-0.9492.hdf5\n",
      "\n",
      "Epoch 31: saving model to ./data/model/all/31-0.9469.hdf5\n",
      "\n",
      "Epoch 32: saving model to ./data/model/all/32-0.9508.hdf5\n",
      "\n",
      "Epoch 33: saving model to ./data/model/all/33-0.9515.hdf5\n",
      "\n",
      "Epoch 34: saving model to ./data/model/all/34-0.9515.hdf5\n",
      "\n",
      "Epoch 35: saving model to ./data/model/all/35-0.9523.hdf5\n",
      "\n",
      "Epoch 36: saving model to ./data/model/all/36-0.9531.hdf5\n",
      "\n",
      "Epoch 37: saving model to ./data/model/all/37-0.9523.hdf5\n",
      "\n",
      "Epoch 38: saving model to ./data/model/all/38-0.9531.hdf5\n",
      "\n",
      "Epoch 39: saving model to ./data/model/all/39-0.9531.hdf5\n",
      "\n",
      "Epoch 40: saving model to ./data/model/all/40-0.9538.hdf5\n",
      "\n",
      "Epoch 41: saving model to ./data/model/all/41-0.9538.hdf5\n",
      "\n",
      "Epoch 42: saving model to ./data/model/all/42-0.9508.hdf5\n",
      "\n",
      "Epoch 43: saving model to ./data/model/all/43-0.9531.hdf5\n",
      "\n",
      "Epoch 44: saving model to ./data/model/all/44-0.9538.hdf5\n",
      "\n",
      "Epoch 45: saving model to ./data/model/all/45-0.9546.hdf5\n",
      "\n",
      "Epoch 46: saving model to ./data/model/all/46-0.9523.hdf5\n",
      "\n",
      "Epoch 47: saving model to ./data/model/all/47-0.9562.hdf5\n",
      "\n",
      "Epoch 48: saving model to ./data/model/all/48-0.9546.hdf5\n",
      "\n",
      "Epoch 49: saving model to ./data/model/all/49-0.9546.hdf5\n",
      "\n",
      "Epoch 50: saving model to ./data/model/all/50-0.9554.hdf5\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장의 조건을 설정합니다.\n",
    "modelpath=\"./data/model/all/{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)\n",
    "\n",
    "# 모델을 실행합니다. \n",
    "history=model.fit(X_train, y_train, epochs=50, batch_size=500, validation_split=0.25, verbose=0, callbacks=[checkpointer])\n",
    "\n",
    "# 에포크가 진행됨에 따라서 설정된 폴더 안에 설정된 파일로 저장이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 795us/step - loss: 0.1144 - accuracy: 0.9669\n",
      "Test accuracy: 0.9669230580329895\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 그래프로 과적합 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 확인을 위한 긴 학습 (컴퓨터 환경에 따라 시간이 다소 걸릴수 있습니다)\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.140837</td>\n",
       "      <td>0.945856</td>\n",
       "      <td>0.134746</td>\n",
       "      <td>0.956923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137930</td>\n",
       "      <td>0.948678</td>\n",
       "      <td>0.135519</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.138421</td>\n",
       "      <td>0.945599</td>\n",
       "      <td>0.133190</td>\n",
       "      <td>0.956923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.135970</td>\n",
       "      <td>0.947395</td>\n",
       "      <td>0.132998</td>\n",
       "      <td>0.956923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135558</td>\n",
       "      <td>0.948935</td>\n",
       "      <td>0.132034</td>\n",
       "      <td>0.955385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.038308</td>\n",
       "      <td>0.991275</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.986923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.038247</td>\n",
       "      <td>0.990249</td>\n",
       "      <td>0.054958</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.037534</td>\n",
       "      <td>0.990506</td>\n",
       "      <td>0.055291</td>\n",
       "      <td>0.987692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.037790</td>\n",
       "      <td>0.990249</td>\n",
       "      <td>0.059731</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.042245</td>\n",
       "      <td>0.988709</td>\n",
       "      <td>0.060974</td>\n",
       "      <td>0.985385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          loss  accuracy  val_loss  val_accuracy\n",
       "0     0.140837  0.945856  0.134746      0.956923\n",
       "1     0.137930  0.948678  0.135519      0.953846\n",
       "2     0.138421  0.945599  0.133190      0.956923\n",
       "3     0.135970  0.947395  0.132998      0.956923\n",
       "4     0.135558  0.948935  0.132034      0.955385\n",
       "...        ...       ...       ...           ...\n",
       "1995  0.038308  0.991275  0.056295      0.986923\n",
       "1996  0.038247  0.990249  0.054958      0.987692\n",
       "1997  0.037534  0.990506  0.055291      0.987692\n",
       "1998  0.037790  0.990249  0.059731      0.985385\n",
       "1999  0.042245  0.988709  0.060974      0.985385\n",
       "\n",
       "[2000 rows x 4 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history에 저장된 학습 결과를 확인해 보겠습니다. \n",
    "hist_df=pd.DataFrame(history.history)\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_vloss에 테스트셋(여기서는 검증셋)의 오차를 저장합니다.\n",
    "y_vloss=hist_df['val_loss']\n",
    "\n",
    "# y_loss에 학습셋의 오차를 저장합니다.\n",
    "y_loss=hist_df['loss']\n",
    "\n",
    "# x 값을 지정하고 테스트셋(검증셋)의 오차를 빨간색으로, 학습셋의 오차를 파란색으로 표시합니다.\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2, label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2, label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습의 자동 중단"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 코드 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_125 (Dense)           (None, 30)                390       \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 12)                372       \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 875\n",
      "Trainable params: 875\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터를 입력합니다.\n",
    "df = pd.read_csv('./data/wine.csv', header=None)\n",
    "\n",
    "# 와인의 속성을 X로 와인의 분류를 y로 저장합니다.\n",
    "X = df.iloc[:,0:12]\n",
    "y = df.iloc[:,12]\n",
    "\n",
    "#학습셋과 테스트셋으로 나눕니다.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "# 모델 구조를 설정합니다.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,  input_dim=12, activation='relu'))\n",
    "model.add(Dense(12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "#모델을 컴파일합니다.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습의 자동 중단 및 최적화 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 2.0025 - accuracy: 0.5525 - val_loss: 1.0078 - val_accuracy: 0.7846\n",
      "Epoch 2/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.2138 - accuracy: 0.7626 - val_loss: 1.0970 - val_accuracy: 0.7938\n",
      "Epoch 3/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 1.0153 - accuracy: 0.7816 - val_loss: 0.6459 - val_accuracy: 0.8269\n",
      "Epoch 4/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4967 - accuracy: 0.8355 - val_loss: 0.4343 - val_accuracy: 0.7969\n",
      "Epoch 5/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4175 - accuracy: 0.7965 - val_loss: 0.3219 - val_accuracy: 0.8854\n",
      "Epoch 6/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8814 - val_loss: 0.3170 - val_accuracy: 0.8985\n",
      "Epoch 7/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2971 - accuracy: 0.8909 - val_loss: 0.2945 - val_accuracy: 0.8854\n",
      "Epoch 8/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2807 - accuracy: 0.8894 - val_loss: 0.2775 - val_accuracy: 0.8969\n",
      "Epoch 9/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2663 - accuracy: 0.9030 - val_loss: 0.2675 - val_accuracy: 0.9069\n",
      "Epoch 10/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2574 - accuracy: 0.9071 - val_loss: 0.2615 - val_accuracy: 0.8954\n",
      "Epoch 11/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2480 - accuracy: 0.9097 - val_loss: 0.2502 - val_accuracy: 0.9085\n",
      "Epoch 12/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2390 - accuracy: 0.9171 - val_loss: 0.2398 - val_accuracy: 0.9169\n",
      "Epoch 13/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2299 - accuracy: 0.9210 - val_loss: 0.2344 - val_accuracy: 0.9200\n",
      "Epoch 14/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2262 - accuracy: 0.9246 - val_loss: 0.2331 - val_accuracy: 0.9231\n",
      "Epoch 15/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2224 - accuracy: 0.9261 - val_loss: 0.2291 - val_accuracy: 0.9238\n",
      "Epoch 16/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2191 - accuracy: 0.9261 - val_loss: 0.2274 - val_accuracy: 0.9254\n",
      "Epoch 17/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2160 - accuracy: 0.9266 - val_loss: 0.2242 - val_accuracy: 0.9254\n",
      "Epoch 18/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2136 - accuracy: 0.9274 - val_loss: 0.2213 - val_accuracy: 0.9285\n",
      "Epoch 19/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2114 - accuracy: 0.9274 - val_loss: 0.2195 - val_accuracy: 0.9285\n",
      "Epoch 20/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2092 - accuracy: 0.9279 - val_loss: 0.2164 - val_accuracy: 0.9285\n",
      "Epoch 21/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2082 - accuracy: 0.9281 - val_loss: 0.2152 - val_accuracy: 0.9292\n",
      "Epoch 22/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2055 - accuracy: 0.9297 - val_loss: 0.2129 - val_accuracy: 0.9300\n",
      "Epoch 23/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2048 - accuracy: 0.9297 - val_loss: 0.2114 - val_accuracy: 0.9300\n",
      "Epoch 24/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2040 - accuracy: 0.9287 - val_loss: 0.2092 - val_accuracy: 0.9300\n",
      "Epoch 25/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2036 - accuracy: 0.9312 - val_loss: 0.2073 - val_accuracy: 0.9308\n",
      "Epoch 26/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.2007 - accuracy: 0.9284 - val_loss: 0.2066 - val_accuracy: 0.9315\n",
      "Epoch 27/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1986 - accuracy: 0.9310 - val_loss: 0.2044 - val_accuracy: 0.9315\n",
      "Epoch 28/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1981 - accuracy: 0.9307 - val_loss: 0.2042 - val_accuracy: 0.9315\n",
      "Epoch 29/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1961 - accuracy: 0.9312 - val_loss: 0.2018 - val_accuracy: 0.9323\n",
      "Epoch 30/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1948 - accuracy: 0.9315 - val_loss: 0.2017 - val_accuracy: 0.9308\n",
      "Epoch 31/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1942 - accuracy: 0.9315 - val_loss: 0.1995 - val_accuracy: 0.9323\n",
      "Epoch 32/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1930 - accuracy: 0.9317 - val_loss: 0.1984 - val_accuracy: 0.9315\n",
      "Epoch 33/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1923 - accuracy: 0.9315 - val_loss: 0.1974 - val_accuracy: 0.9315\n",
      "Epoch 34/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1918 - accuracy: 0.9315 - val_loss: 0.1965 - val_accuracy: 0.9315\n",
      "Epoch 35/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1909 - accuracy: 0.9320 - val_loss: 0.1959 - val_accuracy: 0.9323\n",
      "Epoch 36/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1897 - accuracy: 0.9325 - val_loss: 0.1946 - val_accuracy: 0.9331\n",
      "Epoch 37/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1890 - accuracy: 0.9325 - val_loss: 0.1940 - val_accuracy: 0.9331\n",
      "Epoch 38/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1881 - accuracy: 0.9325 - val_loss: 0.1938 - val_accuracy: 0.9331\n",
      "Epoch 39/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1872 - accuracy: 0.9333 - val_loss: 0.1921 - val_accuracy: 0.9331\n",
      "Epoch 40/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1869 - accuracy: 0.9333 - val_loss: 0.1915 - val_accuracy: 0.9331\n",
      "Epoch 41/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1859 - accuracy: 0.9338 - val_loss: 0.1911 - val_accuracy: 0.9338\n",
      "Epoch 42/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1855 - accuracy: 0.9341 - val_loss: 0.1898 - val_accuracy: 0.9338\n",
      "Epoch 43/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1856 - accuracy: 0.9330 - val_loss: 0.1904 - val_accuracy: 0.9331\n",
      "Epoch 44/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1847 - accuracy: 0.9343 - val_loss: 0.1883 - val_accuracy: 0.9331\n",
      "Epoch 45/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1829 - accuracy: 0.9343 - val_loss: 0.1880 - val_accuracy: 0.9338\n",
      "Epoch 46/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1821 - accuracy: 0.9348 - val_loss: 0.1866 - val_accuracy: 0.9338\n",
      "Epoch 47/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9356 - val_loss: 0.1867 - val_accuracy: 0.9338\n",
      "Epoch 48/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1807 - accuracy: 0.9353 - val_loss: 0.1852 - val_accuracy: 0.9338\n",
      "Epoch 49/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9358 - val_loss: 0.1845 - val_accuracy: 0.9338\n",
      "Epoch 50/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1792 - accuracy: 0.9356 - val_loss: 0.1842 - val_accuracy: 0.9338\n",
      "Epoch 51/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1785 - accuracy: 0.9364 - val_loss: 0.1830 - val_accuracy: 0.9338\n",
      "Epoch 52/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1780 - accuracy: 0.9358 - val_loss: 0.1821 - val_accuracy: 0.9338\n",
      "Epoch 53/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1771 - accuracy: 0.9366 - val_loss: 0.1813 - val_accuracy: 0.9338\n",
      "Epoch 54/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1766 - accuracy: 0.9374 - val_loss: 0.1808 - val_accuracy: 0.9369\n",
      "Epoch 55/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1757 - accuracy: 0.9382 - val_loss: 0.1797 - val_accuracy: 0.9346\n",
      "Epoch 56/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1749 - accuracy: 0.9384 - val_loss: 0.1790 - val_accuracy: 0.9385\n",
      "Epoch 57/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9369 - val_loss: 0.1780 - val_accuracy: 0.9377\n",
      "Epoch 58/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1733 - accuracy: 0.9392 - val_loss: 0.1786 - val_accuracy: 0.9392\n",
      "Epoch 59/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1729 - accuracy: 0.9400 - val_loss: 0.1762 - val_accuracy: 0.9369\n",
      "Epoch 60/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9397 - val_loss: 0.1776 - val_accuracy: 0.9408\n",
      "Epoch 61/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1719 - accuracy: 0.9400 - val_loss: 0.1745 - val_accuracy: 0.9400\n",
      "Epoch 62/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1707 - accuracy: 0.9402 - val_loss: 0.1737 - val_accuracy: 0.9408\n",
      "Epoch 63/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1705 - accuracy: 0.9397 - val_loss: 0.1740 - val_accuracy: 0.9415\n",
      "Epoch 64/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1692 - accuracy: 0.9402 - val_loss: 0.1718 - val_accuracy: 0.9408\n",
      "Epoch 65/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1687 - accuracy: 0.9402 - val_loss: 0.1709 - val_accuracy: 0.9423\n",
      "Epoch 66/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1676 - accuracy: 0.9415 - val_loss: 0.1708 - val_accuracy: 0.9423\n",
      "Epoch 67/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.9412 - val_loss: 0.1691 - val_accuracy: 0.9415\n",
      "Epoch 68/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1664 - accuracy: 0.9412 - val_loss: 0.1685 - val_accuracy: 0.9423\n",
      "Epoch 69/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1655 - accuracy: 0.9415 - val_loss: 0.1675 - val_accuracy: 0.9423\n",
      "Epoch 70/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1646 - accuracy: 0.9418 - val_loss: 0.1668 - val_accuracy: 0.9423\n",
      "Epoch 71/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1635 - accuracy: 0.9420 - val_loss: 0.1673 - val_accuracy: 0.9423\n",
      "Epoch 72/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9420 - val_loss: 0.1649 - val_accuracy: 0.9423\n",
      "Epoch 73/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9425 - val_loss: 0.1660 - val_accuracy: 0.9438\n",
      "Epoch 74/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1630 - accuracy: 0.9420 - val_loss: 0.1635 - val_accuracy: 0.9431\n",
      "Epoch 75/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1616 - accuracy: 0.9423 - val_loss: 0.1625 - val_accuracy: 0.9438\n",
      "Epoch 76/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1622 - accuracy: 0.9428 - val_loss: 0.1644 - val_accuracy: 0.9438\n",
      "Epoch 77/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1607 - accuracy: 0.9420 - val_loss: 0.1609 - val_accuracy: 0.9438\n",
      "Epoch 78/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9438 - val_loss: 0.1607 - val_accuracy: 0.9415\n",
      "Epoch 79/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1599 - accuracy: 0.9435 - val_loss: 0.1591 - val_accuracy: 0.9446\n",
      "Epoch 80/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1581 - accuracy: 0.9446 - val_loss: 0.1591 - val_accuracy: 0.9446\n",
      "Epoch 81/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9446 - val_loss: 0.1577 - val_accuracy: 0.9431\n",
      "Epoch 82/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1570 - accuracy: 0.9448 - val_loss: 0.1568 - val_accuracy: 0.9446\n",
      "Epoch 83/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1571 - accuracy: 0.9453 - val_loss: 0.1577 - val_accuracy: 0.9454\n",
      "Epoch 84/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1562 - accuracy: 0.9448 - val_loss: 0.1552 - val_accuracy: 0.9446\n",
      "Epoch 85/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9446 - val_loss: 0.1544 - val_accuracy: 0.9446\n",
      "Epoch 86/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1567 - accuracy: 0.9438 - val_loss: 0.1542 - val_accuracy: 0.9462\n",
      "Epoch 87/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1536 - accuracy: 0.9464 - val_loss: 0.1531 - val_accuracy: 0.9469\n",
      "Epoch 88/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9448 - val_loss: 0.1526 - val_accuracy: 0.9431\n",
      "Epoch 89/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9453 - val_loss: 0.1507 - val_accuracy: 0.9469\n",
      "Epoch 90/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1533 - accuracy: 0.9464 - val_loss: 0.1532 - val_accuracy: 0.9477\n",
      "Epoch 91/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1529 - accuracy: 0.9448 - val_loss: 0.1496 - val_accuracy: 0.9438\n",
      "Epoch 92/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9459 - val_loss: 0.1486 - val_accuracy: 0.9492\n",
      "Epoch 93/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1500 - accuracy: 0.9456 - val_loss: 0.1479 - val_accuracy: 0.9469\n",
      "Epoch 94/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9456 - val_loss: 0.1466 - val_accuracy: 0.9454\n",
      "Epoch 95/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1483 - accuracy: 0.9459 - val_loss: 0.1462 - val_accuracy: 0.9431\n",
      "Epoch 96/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9477 - val_loss: 0.1468 - val_accuracy: 0.9469\n",
      "Epoch 97/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9471 - val_loss: 0.1450 - val_accuracy: 0.9438\n",
      "Epoch 98/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9474 - val_loss: 0.1433 - val_accuracy: 0.9477\n",
      "Epoch 99/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1451 - accuracy: 0.9469 - val_loss: 0.1431 - val_accuracy: 0.9485\n",
      "Epoch 100/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9477 - val_loss: 0.1420 - val_accuracy: 0.9477\n",
      "Epoch 101/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1437 - accuracy: 0.9479 - val_loss: 0.1419 - val_accuracy: 0.9438\n",
      "Epoch 102/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1442 - accuracy: 0.9487 - val_loss: 0.1416 - val_accuracy: 0.9485\n",
      "Epoch 103/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9487 - val_loss: 0.1397 - val_accuracy: 0.9485\n",
      "Epoch 104/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1419 - accuracy: 0.9469 - val_loss: 0.1386 - val_accuracy: 0.9477\n",
      "Epoch 105/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1422 - accuracy: 0.9482 - val_loss: 0.1396 - val_accuracy: 0.9485\n",
      "Epoch 106/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1409 - accuracy: 0.9479 - val_loss: 0.1373 - val_accuracy: 0.9485\n",
      "Epoch 107/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1396 - accuracy: 0.9484 - val_loss: 0.1361 - val_accuracy: 0.9485\n",
      "Epoch 108/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1387 - accuracy: 0.9484 - val_loss: 0.1363 - val_accuracy: 0.9477\n",
      "Epoch 109/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9479 - val_loss: 0.1342 - val_accuracy: 0.9485\n",
      "Epoch 110/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1375 - accuracy: 0.9484 - val_loss: 0.1335 - val_accuracy: 0.9462\n",
      "Epoch 111/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1381 - accuracy: 0.9482 - val_loss: 0.1348 - val_accuracy: 0.9485\n",
      "Epoch 112/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1376 - accuracy: 0.9487 - val_loss: 0.1337 - val_accuracy: 0.9485\n",
      "Epoch 113/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1355 - accuracy: 0.9494 - val_loss: 0.1328 - val_accuracy: 0.9462\n",
      "Epoch 114/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1353 - accuracy: 0.9487 - val_loss: 0.1330 - val_accuracy: 0.9477\n",
      "Epoch 115/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9510 - val_loss: 0.1304 - val_accuracy: 0.9485\n",
      "Epoch 116/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1339 - accuracy: 0.9494 - val_loss: 0.1294 - val_accuracy: 0.9500\n",
      "Epoch 117/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1328 - accuracy: 0.9512 - val_loss: 0.1301 - val_accuracy: 0.9485\n",
      "Epoch 118/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1323 - accuracy: 0.9502 - val_loss: 0.1279 - val_accuracy: 0.9492\n",
      "Epoch 119/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9507 - val_loss: 0.1260 - val_accuracy: 0.9508\n",
      "Epoch 120/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1306 - accuracy: 0.9507 - val_loss: 0.1259 - val_accuracy: 0.9515\n",
      "Epoch 121/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9510 - val_loss: 0.1257 - val_accuracy: 0.9500\n",
      "Epoch 122/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9512 - val_loss: 0.1254 - val_accuracy: 0.9500\n",
      "Epoch 123/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9525 - val_loss: 0.1242 - val_accuracy: 0.9515\n",
      "Epoch 124/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1302 - accuracy: 0.9505 - val_loss: 0.1242 - val_accuracy: 0.9508\n",
      "Epoch 125/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9528 - val_loss: 0.1265 - val_accuracy: 0.9538\n",
      "Epoch 126/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1271 - accuracy: 0.9518 - val_loss: 0.1211 - val_accuracy: 0.9515\n",
      "Epoch 127/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1254 - accuracy: 0.9523 - val_loss: 0.1211 - val_accuracy: 0.9515\n",
      "Epoch 128/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1252 - accuracy: 0.9523 - val_loss: 0.1201 - val_accuracy: 0.9531\n",
      "Epoch 129/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1257 - accuracy: 0.9543 - val_loss: 0.1210 - val_accuracy: 0.9508\n",
      "Epoch 130/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1257 - accuracy: 0.9533 - val_loss: 0.1180 - val_accuracy: 0.9523\n",
      "Epoch 131/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1237 - accuracy: 0.9528 - val_loss: 0.1172 - val_accuracy: 0.9538\n",
      "Epoch 132/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1218 - accuracy: 0.9543 - val_loss: 0.1164 - val_accuracy: 0.9523\n",
      "Epoch 133/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1214 - accuracy: 0.9551 - val_loss: 0.1159 - val_accuracy: 0.9531\n",
      "Epoch 134/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1215 - accuracy: 0.9546 - val_loss: 0.1146 - val_accuracy: 0.9538\n",
      "Epoch 135/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1209 - accuracy: 0.9551 - val_loss: 0.1150 - val_accuracy: 0.9562\n",
      "Epoch 136/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1195 - accuracy: 0.9548 - val_loss: 0.1130 - val_accuracy: 0.9554\n",
      "Epoch 137/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1184 - accuracy: 0.9561 - val_loss: 0.1137 - val_accuracy: 0.9546\n",
      "Epoch 138/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1199 - accuracy: 0.9538 - val_loss: 0.1165 - val_accuracy: 0.9546\n",
      "Epoch 139/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9551 - val_loss: 0.1147 - val_accuracy: 0.9562\n",
      "Epoch 140/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9530 - val_loss: 0.1101 - val_accuracy: 0.9554\n",
      "Epoch 141/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1175 - accuracy: 0.9569 - val_loss: 0.1089 - val_accuracy: 0.9569\n",
      "Epoch 142/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1176 - accuracy: 0.9584 - val_loss: 0.1179 - val_accuracy: 0.9523\n",
      "Epoch 143/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9571 - val_loss: 0.1086 - val_accuracy: 0.9562\n",
      "Epoch 144/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1140 - accuracy: 0.9566 - val_loss: 0.1086 - val_accuracy: 0.9592\n",
      "Epoch 145/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.9569 - val_loss: 0.1089 - val_accuracy: 0.9600\n",
      "Epoch 146/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1131 - accuracy: 0.9579 - val_loss: 0.1066 - val_accuracy: 0.9554\n",
      "Epoch 147/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9584 - val_loss: 0.1049 - val_accuracy: 0.9569\n",
      "Epoch 148/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9574 - val_loss: 0.1056 - val_accuracy: 0.9592\n",
      "Epoch 149/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1107 - accuracy: 0.9577 - val_loss: 0.1034 - val_accuracy: 0.9569\n",
      "Epoch 150/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1120 - accuracy: 0.9574 - val_loss: 0.1053 - val_accuracy: 0.9554\n",
      "Epoch 151/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1126 - accuracy: 0.9587 - val_loss: 0.1038 - val_accuracy: 0.9562\n",
      "Epoch 152/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9584 - val_loss: 0.1010 - val_accuracy: 0.9569\n",
      "Epoch 153/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9607 - val_loss: 0.1001 - val_accuracy: 0.9592\n",
      "Epoch 154/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9589 - val_loss: 0.1045 - val_accuracy: 0.9623\n",
      "Epoch 155/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9587 - val_loss: 0.0985 - val_accuracy: 0.9600\n",
      "Epoch 156/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1079 - accuracy: 0.9607 - val_loss: 0.0975 - val_accuracy: 0.9577\n",
      "Epoch 157/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1053 - accuracy: 0.9613 - val_loss: 0.0967 - val_accuracy: 0.9600\n",
      "Epoch 158/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1056 - accuracy: 0.9587 - val_loss: 0.0989 - val_accuracy: 0.9638\n",
      "Epoch 159/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1055 - accuracy: 0.9605 - val_loss: 0.0988 - val_accuracy: 0.9646\n",
      "Epoch 160/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9620 - val_loss: 0.0943 - val_accuracy: 0.9608\n",
      "Epoch 161/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1065 - accuracy: 0.9615 - val_loss: 0.0962 - val_accuracy: 0.9585\n",
      "Epoch 162/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1057 - accuracy: 0.9605 - val_loss: 0.0953 - val_accuracy: 0.9600\n",
      "Epoch 163/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1025 - accuracy: 0.9628 - val_loss: 0.0916 - val_accuracy: 0.9608\n",
      "Epoch 164/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1026 - accuracy: 0.9625 - val_loss: 0.0918 - val_accuracy: 0.9638\n",
      "Epoch 165/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1004 - accuracy: 0.9620 - val_loss: 0.0903 - val_accuracy: 0.9623\n",
      "Epoch 166/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9620 - val_loss: 0.0903 - val_accuracy: 0.9638\n",
      "Epoch 167/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1022 - accuracy: 0.9625 - val_loss: 0.0907 - val_accuracy: 0.9662\n",
      "Epoch 168/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9620 - val_loss: 0.0922 - val_accuracy: 0.9654\n",
      "Epoch 169/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9641 - val_loss: 0.0896 - val_accuracy: 0.9677\n",
      "Epoch 170/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9659 - val_loss: 0.0913 - val_accuracy: 0.9608\n",
      "Epoch 171/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9643 - val_loss: 0.0882 - val_accuracy: 0.9677\n",
      "Epoch 172/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.9646 - val_loss: 0.0860 - val_accuracy: 0.9669\n",
      "Epoch 173/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0977 - accuracy: 0.9633 - val_loss: 0.0853 - val_accuracy: 0.9677\n",
      "Epoch 174/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0994 - accuracy: 0.9648 - val_loss: 0.0870 - val_accuracy: 0.9615\n",
      "Epoch 175/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9672 - val_loss: 0.0848 - val_accuracy: 0.9654\n",
      "Epoch 176/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0948 - accuracy: 0.9654 - val_loss: 0.0835 - val_accuracy: 0.9669\n",
      "Epoch 177/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0947 - accuracy: 0.9661 - val_loss: 0.0828 - val_accuracy: 0.9692\n",
      "Epoch 178/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9654 - val_loss: 0.0843 - val_accuracy: 0.9692\n",
      "Epoch 179/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0935 - accuracy: 0.9651 - val_loss: 0.0838 - val_accuracy: 0.9685\n",
      "Epoch 180/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.9659 - val_loss: 0.0835 - val_accuracy: 0.9685\n",
      "Epoch 181/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9669 - val_loss: 0.0835 - val_accuracy: 0.9708\n",
      "Epoch 182/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9674 - val_loss: 0.0801 - val_accuracy: 0.9700\n",
      "Epoch 183/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 0.9682 - val_loss: 0.0819 - val_accuracy: 0.9654\n",
      "Epoch 184/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0926 - accuracy: 0.9672 - val_loss: 0.0795 - val_accuracy: 0.9708\n",
      "Epoch 185/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9697 - val_loss: 0.0816 - val_accuracy: 0.9654\n",
      "Epoch 186/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0940 - accuracy: 0.9651 - val_loss: 0.0810 - val_accuracy: 0.9646\n",
      "Epoch 187/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0921 - accuracy: 0.9661 - val_loss: 0.0773 - val_accuracy: 0.9708\n",
      "Epoch 188/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9677 - val_loss: 0.0796 - val_accuracy: 0.9738\n",
      "Epoch 189/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9684 - val_loss: 0.0767 - val_accuracy: 0.9708\n",
      "Epoch 190/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9682 - val_loss: 0.0774 - val_accuracy: 0.9708\n",
      "Epoch 191/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0877 - accuracy: 0.9713 - val_loss: 0.0760 - val_accuracy: 0.9708\n",
      "Epoch 192/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0866 - accuracy: 0.9715 - val_loss: 0.0776 - val_accuracy: 0.9746\n",
      "Epoch 193/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0874 - accuracy: 0.9695 - val_loss: 0.0764 - val_accuracy: 0.9738\n",
      "Epoch 194/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0855 - accuracy: 0.9710 - val_loss: 0.0737 - val_accuracy: 0.9708\n",
      "Epoch 195/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9713 - val_loss: 0.0737 - val_accuracy: 0.9708\n",
      "Epoch 196/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9705 - val_loss: 0.0733 - val_accuracy: 0.9708\n",
      "Epoch 197/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0845 - accuracy: 0.9710 - val_loss: 0.0730 - val_accuracy: 0.9738\n",
      "Epoch 198/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.9723 - val_loss: 0.0717 - val_accuracy: 0.9723\n",
      "Epoch 199/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9725 - val_loss: 0.0712 - val_accuracy: 0.9731\n",
      "Epoch 200/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9710 - val_loss: 0.0724 - val_accuracy: 0.9754\n",
      "Epoch 201/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9702 - val_loss: 0.0721 - val_accuracy: 0.9754\n",
      "Epoch 202/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0823 - accuracy: 0.9718 - val_loss: 0.0701 - val_accuracy: 0.9746\n",
      "Epoch 203/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0819 - accuracy: 0.9713 - val_loss: 0.0728 - val_accuracy: 0.9746\n",
      "Epoch 204/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9725 - val_loss: 0.0703 - val_accuracy: 0.9746\n",
      "Epoch 205/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9720 - val_loss: 0.0684 - val_accuracy: 0.9738\n",
      "Epoch 206/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9723 - val_loss: 0.0686 - val_accuracy: 0.9746\n",
      "Epoch 207/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0821 - accuracy: 0.9713 - val_loss: 0.0691 - val_accuracy: 0.9754\n",
      "Epoch 208/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9690 - val_loss: 0.0681 - val_accuracy: 0.9746\n",
      "Epoch 209/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9749 - val_loss: 0.0687 - val_accuracy: 0.9715\n",
      "Epoch 210/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9731 - val_loss: 0.0666 - val_accuracy: 0.9738\n",
      "Epoch 211/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0799 - accuracy: 0.9733 - val_loss: 0.0679 - val_accuracy: 0.9715\n",
      "Epoch 212/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9725 - val_loss: 0.0671 - val_accuracy: 0.9762\n",
      "Epoch 213/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0796 - accuracy: 0.9725 - val_loss: 0.0669 - val_accuracy: 0.9762\n",
      "Epoch 214/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9741 - val_loss: 0.0648 - val_accuracy: 0.9762\n",
      "Epoch 215/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9743 - val_loss: 0.0661 - val_accuracy: 0.9762\n",
      "Epoch 216/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9741 - val_loss: 0.0651 - val_accuracy: 0.9754\n",
      "Epoch 217/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9746 - val_loss: 0.0668 - val_accuracy: 0.9754\n",
      "Epoch 218/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9741 - val_loss: 0.0673 - val_accuracy: 0.9762\n",
      "Epoch 219/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0772 - accuracy: 0.9733 - val_loss: 0.0683 - val_accuracy: 0.9762\n",
      "Epoch 220/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0799 - accuracy: 0.9731 - val_loss: 0.0628 - val_accuracy: 0.9754\n",
      "Epoch 221/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9766 - val_loss: 0.0625 - val_accuracy: 0.9769\n",
      "Epoch 222/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9751 - val_loss: 0.0623 - val_accuracy: 0.9785\n",
      "Epoch 223/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0742 - accuracy: 0.9756 - val_loss: 0.0655 - val_accuracy: 0.9754\n",
      "Epoch 224/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9731 - val_loss: 0.0630 - val_accuracy: 0.9762\n",
      "Epoch 225/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9754 - val_loss: 0.0614 - val_accuracy: 0.9762\n",
      "Epoch 226/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0740 - accuracy: 0.9749 - val_loss: 0.0607 - val_accuracy: 0.9785\n",
      "Epoch 227/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0762 - accuracy: 0.9738 - val_loss: 0.0662 - val_accuracy: 0.9746\n",
      "Epoch 228/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9738 - val_loss: 0.0598 - val_accuracy: 0.9800\n",
      "Epoch 229/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0746 - accuracy: 0.9766 - val_loss: 0.0598 - val_accuracy: 0.9785\n",
      "Epoch 230/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0751 - accuracy: 0.9746 - val_loss: 0.0612 - val_accuracy: 0.9762\n",
      "Epoch 231/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9759 - val_loss: 0.0599 - val_accuracy: 0.9769\n",
      "Epoch 232/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9761 - val_loss: 0.0607 - val_accuracy: 0.9800\n",
      "Epoch 233/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9774 - val_loss: 0.0628 - val_accuracy: 0.9769\n",
      "Epoch 234/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9746 - val_loss: 0.0601 - val_accuracy: 0.9762\n",
      "Epoch 235/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0711 - accuracy: 0.9751 - val_loss: 0.0579 - val_accuracy: 0.9800\n",
      "Epoch 236/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0718 - accuracy: 0.9772 - val_loss: 0.0577 - val_accuracy: 0.9823\n",
      "Epoch 237/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0725 - accuracy: 0.9764 - val_loss: 0.0572 - val_accuracy: 0.9808\n",
      "Epoch 238/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9731 - val_loss: 0.0569 - val_accuracy: 0.9815\n",
      "Epoch 239/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9759 - val_loss: 0.0576 - val_accuracy: 0.9800\n",
      "Epoch 240/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9766 - val_loss: 0.0610 - val_accuracy: 0.9785\n",
      "Epoch 241/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9754 - val_loss: 0.0564 - val_accuracy: 0.9808\n",
      "Epoch 242/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9772 - val_loss: 0.0569 - val_accuracy: 0.9815\n",
      "Epoch 243/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9764 - val_loss: 0.0610 - val_accuracy: 0.9792\n",
      "Epoch 244/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0723 - accuracy: 0.9756 - val_loss: 0.0573 - val_accuracy: 0.9823\n",
      "Epoch 245/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9774 - val_loss: 0.0612 - val_accuracy: 0.9777\n",
      "Epoch 246/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0738 - accuracy: 0.9756 - val_loss: 0.0558 - val_accuracy: 0.9785\n",
      "Epoch 247/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9764 - val_loss: 0.0570 - val_accuracy: 0.9762\n",
      "Epoch 248/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0700 - accuracy: 0.9772 - val_loss: 0.0599 - val_accuracy: 0.9785\n",
      "Epoch 249/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0697 - accuracy: 0.9754 - val_loss: 0.0621 - val_accuracy: 0.9800\n",
      "Epoch 250/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0717 - accuracy: 0.9756 - val_loss: 0.0621 - val_accuracy: 0.9815\n",
      "Epoch 251/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0721 - accuracy: 0.9779 - val_loss: 0.0655 - val_accuracy: 0.9815\n",
      "Epoch 252/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0750 - accuracy: 0.9741 - val_loss: 0.0583 - val_accuracy: 0.9808\n",
      "Epoch 253/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9777 - val_loss: 0.0623 - val_accuracy: 0.9815\n",
      "Epoch 254/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0698 - accuracy: 0.9795 - val_loss: 0.0558 - val_accuracy: 0.9792\n",
      "Epoch 255/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.9777 - val_loss: 0.0560 - val_accuracy: 0.9808\n",
      "Epoch 256/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9792 - val_loss: 0.0537 - val_accuracy: 0.9831\n",
      "Epoch 257/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9766 - val_loss: 0.0531 - val_accuracy: 0.9823\n",
      "Epoch 258/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9782 - val_loss: 0.0531 - val_accuracy: 0.9823\n",
      "Epoch 259/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9774 - val_loss: 0.0538 - val_accuracy: 0.9808\n",
      "Epoch 260/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0714 - accuracy: 0.9772 - val_loss: 0.0529 - val_accuracy: 0.9800\n",
      "Epoch 261/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0696 - accuracy: 0.9777 - val_loss: 0.0527 - val_accuracy: 0.9800\n",
      "Epoch 262/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9784 - val_loss: 0.0541 - val_accuracy: 0.9808\n",
      "Epoch 263/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9779 - val_loss: 0.0521 - val_accuracy: 0.9831\n",
      "Epoch 264/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 0.0526 - val_accuracy: 0.9808\n",
      "Epoch 265/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9779 - val_loss: 0.0555 - val_accuracy: 0.9823\n",
      "Epoch 266/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9782 - val_loss: 0.0527 - val_accuracy: 0.9838\n",
      "Epoch 267/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 0.9784 - val_loss: 0.0510 - val_accuracy: 0.9831\n",
      "Epoch 268/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0664 - accuracy: 0.9774 - val_loss: 0.0524 - val_accuracy: 0.9815\n",
      "Epoch 269/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9782 - val_loss: 0.0553 - val_accuracy: 0.9823\n",
      "Epoch 270/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 0.0548 - val_accuracy: 0.9823\n",
      "Epoch 271/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9784 - val_loss: 0.0537 - val_accuracy: 0.9823\n",
      "Epoch 272/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9761 - val_loss: 0.0506 - val_accuracy: 0.9823\n",
      "Epoch 273/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0644 - accuracy: 0.9802 - val_loss: 0.0511 - val_accuracy: 0.9838\n",
      "Epoch 274/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9790 - val_loss: 0.0552 - val_accuracy: 0.9808\n",
      "Epoch 275/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 0.9766 - val_loss: 0.0503 - val_accuracy: 0.9831\n",
      "Epoch 276/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0655 - accuracy: 0.9787 - val_loss: 0.0550 - val_accuracy: 0.9808\n",
      "Epoch 277/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9782 - val_loss: 0.0580 - val_accuracy: 0.9785\n",
      "Epoch 278/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0741 - accuracy: 0.9746 - val_loss: 0.0582 - val_accuracy: 0.9785\n",
      "Epoch 279/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9715 - val_loss: 0.0501 - val_accuracy: 0.9846\n",
      "Epoch 280/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9766 - val_loss: 0.0687 - val_accuracy: 0.9815\n",
      "Epoch 281/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9766 - val_loss: 0.0523 - val_accuracy: 0.9823\n",
      "Epoch 282/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9802 - val_loss: 0.0496 - val_accuracy: 0.9831\n",
      "Epoch 283/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9802 - val_loss: 0.0499 - val_accuracy: 0.9838\n",
      "Epoch 284/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9805 - val_loss: 0.0495 - val_accuracy: 0.9838\n",
      "Epoch 285/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0634 - accuracy: 0.9800 - val_loss: 0.0492 - val_accuracy: 0.9846\n",
      "Epoch 286/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9802 - val_loss: 0.0486 - val_accuracy: 0.9846\n",
      "Epoch 287/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0645 - accuracy: 0.9782 - val_loss: 0.0485 - val_accuracy: 0.9838\n",
      "Epoch 288/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0647 - accuracy: 0.9790 - val_loss: 0.0483 - val_accuracy: 0.9838\n",
      "Epoch 289/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9784 - val_loss: 0.0487 - val_accuracy: 0.9823\n",
      "Epoch 290/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9797 - val_loss: 0.0506 - val_accuracy: 0.9846\n",
      "Epoch 291/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0631 - accuracy: 0.9792 - val_loss: 0.0556 - val_accuracy: 0.9831\n",
      "Epoch 292/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9797 - val_loss: 0.0491 - val_accuracy: 0.9862\n",
      "Epoch 293/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0627 - accuracy: 0.9805 - val_loss: 0.0489 - val_accuracy: 0.9854\n",
      "Epoch 294/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0493 - val_accuracy: 0.9838\n",
      "Epoch 295/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0628 - accuracy: 0.9797 - val_loss: 0.0484 - val_accuracy: 0.9854\n",
      "Epoch 296/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0632 - accuracy: 0.9802 - val_loss: 0.0478 - val_accuracy: 0.9846\n",
      "Epoch 297/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0630 - accuracy: 0.9805 - val_loss: 0.0474 - val_accuracy: 0.9846\n",
      "Epoch 298/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9802 - val_loss: 0.0490 - val_accuracy: 0.9838\n",
      "Epoch 299/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0641 - accuracy: 0.9784 - val_loss: 0.0473 - val_accuracy: 0.9862\n",
      "Epoch 300/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0646 - accuracy: 0.9795 - val_loss: 0.0466 - val_accuracy: 0.9838\n",
      "Epoch 301/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9823 - val_loss: 0.0512 - val_accuracy: 0.9846\n",
      "Epoch 302/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9813 - val_loss: 0.0511 - val_accuracy: 0.9846\n",
      "Epoch 303/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.0482 - val_accuracy: 0.9869\n",
      "Epoch 304/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0622 - accuracy: 0.9802 - val_loss: 0.0508 - val_accuracy: 0.9854\n",
      "Epoch 305/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9815 - val_loss: 0.0482 - val_accuracy: 0.9854\n",
      "Epoch 306/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0616 - accuracy: 0.9795 - val_loss: 0.0481 - val_accuracy: 0.9869\n",
      "Epoch 307/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9813 - val_loss: 0.0491 - val_accuracy: 0.9846\n",
      "Epoch 308/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9808 - val_loss: 0.0470 - val_accuracy: 0.9854\n",
      "Epoch 309/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.0482 - val_accuracy: 0.9862\n",
      "Epoch 310/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0632 - accuracy: 0.9805 - val_loss: 0.0614 - val_accuracy: 0.9815\n",
      "Epoch 311/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0642 - accuracy: 0.9810 - val_loss: 0.0490 - val_accuracy: 0.9869\n",
      "Epoch 312/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9802 - val_loss: 0.0474 - val_accuracy: 0.9877\n",
      "Epoch 313/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0600 - accuracy: 0.9828 - val_loss: 0.0512 - val_accuracy: 0.9838\n",
      "Epoch 314/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 0.0458 - val_accuracy: 0.9862\n",
      "Epoch 315/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9818 - val_loss: 0.0459 - val_accuracy: 0.9862\n",
      "Epoch 316/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0606 - accuracy: 0.9813 - val_loss: 0.0460 - val_accuracy: 0.9854\n",
      "Epoch 317/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9828 - val_loss: 0.0460 - val_accuracy: 0.9854\n",
      "Epoch 318/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0601 - accuracy: 0.9813 - val_loss: 0.0454 - val_accuracy: 0.9862\n",
      "Epoch 319/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9815 - val_loss: 0.0455 - val_accuracy: 0.9862\n",
      "Epoch 320/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0456 - val_accuracy: 0.9854\n",
      "Epoch 321/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0605 - accuracy: 0.9826 - val_loss: 0.0497 - val_accuracy: 0.9846\n",
      "Epoch 322/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9823 - val_loss: 0.0460 - val_accuracy: 0.9846\n",
      "Epoch 323/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9831 - val_loss: 0.0492 - val_accuracy: 0.9838\n",
      "Epoch 324/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9831 - val_loss: 0.0463 - val_accuracy: 0.9862\n",
      "Epoch 325/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0656 - accuracy: 0.9805 - val_loss: 0.0546 - val_accuracy: 0.9838\n",
      "Epoch 326/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0641 - accuracy: 0.9808 - val_loss: 0.0621 - val_accuracy: 0.9815\n",
      "Epoch 327/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9777 - val_loss: 0.0731 - val_accuracy: 0.9762\n",
      "Epoch 328/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0678 - accuracy: 0.9790 - val_loss: 0.0549 - val_accuracy: 0.9838\n",
      "Epoch 329/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9818 - val_loss: 0.0528 - val_accuracy: 0.9854\n",
      "Epoch 330/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9826 - val_loss: 0.0482 - val_accuracy: 0.9862\n",
      "Epoch 331/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9813 - val_loss: 0.0453 - val_accuracy: 0.9854\n",
      "Epoch 332/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9815 - val_loss: 0.0457 - val_accuracy: 0.9854\n",
      "Epoch 333/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9823 - val_loss: 0.0448 - val_accuracy: 0.9862\n",
      "Epoch 334/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0587 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9885\n",
      "Epoch 335/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.0443 - val_accuracy: 0.9854\n",
      "Epoch 336/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9841 - val_loss: 0.0479 - val_accuracy: 0.9862\n",
      "Epoch 337/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0579 - accuracy: 0.9841 - val_loss: 0.0440 - val_accuracy: 0.9869\n",
      "Epoch 338/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9838 - val_loss: 0.0451 - val_accuracy: 0.9869\n",
      "Epoch 339/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9831 - val_loss: 0.0444 - val_accuracy: 0.9885\n",
      "Epoch 340/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0579 - accuracy: 0.9820 - val_loss: 0.0441 - val_accuracy: 0.9854\n",
      "Epoch 341/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0574 - accuracy: 0.9841 - val_loss: 0.0440 - val_accuracy: 0.9869\n",
      "Epoch 342/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 0.9877\n",
      "Epoch 343/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.0459 - val_accuracy: 0.9900\n",
      "Epoch 344/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 0.9818 - val_loss: 0.0488 - val_accuracy: 0.9869\n",
      "Epoch 345/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 0.0439 - val_accuracy: 0.9846\n",
      "Epoch 346/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9826 - val_loss: 0.0454 - val_accuracy: 0.9877\n",
      "Epoch 347/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9838 - val_loss: 0.0448 - val_accuracy: 0.9862\n",
      "Epoch 348/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9836 - val_loss: 0.0434 - val_accuracy: 0.9854\n",
      "Epoch 349/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0578 - accuracy: 0.9831 - val_loss: 0.0433 - val_accuracy: 0.9869\n",
      "Epoch 350/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9818 - val_loss: 0.0447 - val_accuracy: 0.9869\n",
      "Epoch 351/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.0445 - val_accuracy: 0.9869\n",
      "Epoch 352/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9815 - val_loss: 0.0438 - val_accuracy: 0.9854\n",
      "Epoch 353/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0602 - accuracy: 0.9818 - val_loss: 0.0494 - val_accuracy: 0.9823\n",
      "Epoch 354/2000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9818 - val_loss: 0.0468 - val_accuracy: 0.9846\n",
      "Epoch 355/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 0.9808 - val_loss: 0.0486 - val_accuracy: 0.9838\n",
      "Epoch 356/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9802 - val_loss: 0.0439 - val_accuracy: 0.9877\n",
      "Epoch 357/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0486 - val_accuracy: 0.9877\n",
      "Epoch 358/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9826 - val_loss: 0.0430 - val_accuracy: 0.9846\n",
      "Epoch 359/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.0435 - val_accuracy: 0.9854\n",
      "Epoch 360/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0431 - val_accuracy: 0.9854\n",
      "Epoch 361/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9831 - val_loss: 0.0440 - val_accuracy: 0.9869\n",
      "Epoch 362/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0566 - accuracy: 0.9843 - val_loss: 0.0427 - val_accuracy: 0.9885\n",
      "Epoch 363/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0580 - accuracy: 0.9846 - val_loss: 0.0429 - val_accuracy: 0.9854\n",
      "Epoch 364/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0597 - accuracy: 0.9815 - val_loss: 0.0441 - val_accuracy: 0.9900\n",
      "Epoch 365/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9815 - val_loss: 0.0424 - val_accuracy: 0.9862\n",
      "Epoch 366/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9820 - val_loss: 0.0447 - val_accuracy: 0.9908\n",
      "Epoch 367/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9838 - val_loss: 0.0592 - val_accuracy: 0.9823\n",
      "Epoch 368/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 0.0532 - val_accuracy: 0.9854\n",
      "Epoch 369/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0599 - accuracy: 0.9826 - val_loss: 0.0450 - val_accuracy: 0.9892\n",
      "Epoch 370/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0586 - accuracy: 0.9820 - val_loss: 0.0466 - val_accuracy: 0.9877\n",
      "Epoch 371/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.0440 - val_accuracy: 0.9854\n",
      "Epoch 372/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0430 - val_accuracy: 0.9854\n",
      "Epoch 373/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0557 - accuracy: 0.9841 - val_loss: 0.0424 - val_accuracy: 0.9877\n",
      "Epoch 374/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0548 - accuracy: 0.9843 - val_loss: 0.0432 - val_accuracy: 0.9892\n",
      "Epoch 375/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9836 - val_loss: 0.0489 - val_accuracy: 0.9877\n",
      "Epoch 376/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9838 - val_loss: 0.0445 - val_accuracy: 0.9908\n",
      "Epoch 377/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9836 - val_loss: 0.0452 - val_accuracy: 0.9900\n",
      "Epoch 378/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0548 - accuracy: 0.9836 - val_loss: 0.0420 - val_accuracy: 0.9877\n",
      "Epoch 379/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9841 - val_loss: 0.0437 - val_accuracy: 0.9908\n",
      "Epoch 380/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.0461 - val_accuracy: 0.9908\n",
      "Epoch 381/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9833 - val_loss: 0.0417 - val_accuracy: 0.9854\n",
      "Epoch 382/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0562 - accuracy: 0.9826 - val_loss: 0.0425 - val_accuracy: 0.9892\n",
      "Epoch 383/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0577 - accuracy: 0.9826 - val_loss: 0.0440 - val_accuracy: 0.9900\n",
      "Epoch 384/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0549 - accuracy: 0.9843 - val_loss: 0.0413 - val_accuracy: 0.9877\n",
      "Epoch 385/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0549 - accuracy: 0.9833 - val_loss: 0.0456 - val_accuracy: 0.9854\n",
      "Epoch 386/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0575 - accuracy: 0.9828 - val_loss: 0.0417 - val_accuracy: 0.9854\n",
      "Epoch 387/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9828 - val_loss: 0.0427 - val_accuracy: 0.9869\n",
      "Epoch 388/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9826 - val_loss: 0.0433 - val_accuracy: 0.9854\n",
      "Epoch 389/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9820 - val_loss: 0.0494 - val_accuracy: 0.9831\n",
      "Epoch 390/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0624 - accuracy: 0.9808 - val_loss: 0.0518 - val_accuracy: 0.9823\n",
      "Epoch 391/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0615 - accuracy: 0.9820 - val_loss: 0.0428 - val_accuracy: 0.9862\n",
      "Epoch 392/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 0.0425 - val_accuracy: 0.9892\n",
      "Epoch 393/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9849 - val_loss: 0.0414 - val_accuracy: 0.9892\n",
      "Epoch 394/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9823 - val_loss: 0.0412 - val_accuracy: 0.9869\n",
      "Epoch 395/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0585 - accuracy: 0.9800 - val_loss: 0.0420 - val_accuracy: 0.9854\n",
      "Epoch 396/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0619 - accuracy: 0.9797 - val_loss: 0.0413 - val_accuracy: 0.9854\n",
      "Epoch 397/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9843 - val_loss: 0.0415 - val_accuracy: 0.9869\n",
      "Epoch 398/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9846 - val_loss: 0.0441 - val_accuracy: 0.9846\n",
      "Epoch 399/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0568 - accuracy: 0.9849 - val_loss: 0.0442 - val_accuracy: 0.9846\n",
      "Epoch 400/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9823 - val_loss: 0.0417 - val_accuracy: 0.9854\n",
      "Epoch 401/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0561 - accuracy: 0.9826 - val_loss: 0.0410 - val_accuracy: 0.9862\n",
      "Epoch 402/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9851 - val_loss: 0.0428 - val_accuracy: 0.9908\n",
      "Epoch 403/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.0412 - val_accuracy: 0.9854\n",
      "Epoch 404/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9841 - val_loss: 0.0421 - val_accuracy: 0.9892\n",
      "Epoch 405/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9843 - val_loss: 0.0436 - val_accuracy: 0.9915\n",
      "Epoch 406/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0529 - accuracy: 0.9859 - val_loss: 0.0409 - val_accuracy: 0.9877\n",
      "Epoch 407/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0538 - accuracy: 0.9851 - val_loss: 0.0443 - val_accuracy: 0.9854\n",
      "Epoch 408/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9841 - val_loss: 0.0412 - val_accuracy: 0.9877\n",
      "Epoch 409/2000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0549 - accuracy: 0.9851 - val_loss: 0.0408 - val_accuracy: 0.9885\n",
      "Epoch 410/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0537 - accuracy: 0.9838 - val_loss: 0.0427 - val_accuracy: 0.9908\n",
      "Epoch 411/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0578 - accuracy: 0.9818 - val_loss: 0.0422 - val_accuracy: 0.9900\n",
      "Epoch 412/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.0427 - val_accuracy: 0.9908\n",
      "Epoch 413/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9851 - val_loss: 0.0414 - val_accuracy: 0.9877\n",
      "Epoch 414/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0524 - accuracy: 0.9849 - val_loss: 0.0413 - val_accuracy: 0.9869\n",
      "Epoch 415/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.0435 - val_accuracy: 0.9854\n",
      "Epoch 416/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9820 - val_loss: 0.0409 - val_accuracy: 0.9877\n",
      "Epoch 417/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0542 - accuracy: 0.9838 - val_loss: 0.0410 - val_accuracy: 0.9885\n",
      "Epoch 418/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0554 - accuracy: 0.9833 - val_loss: 0.0419 - val_accuracy: 0.9854\n",
      "Epoch 419/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9810 - val_loss: 0.0418 - val_accuracy: 0.9846\n",
      "Epoch 420/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9838 - val_loss: 0.0421 - val_accuracy: 0.9862\n",
      "Epoch 421/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0582 - accuracy: 0.9823 - val_loss: 0.0420 - val_accuracy: 0.9885\n",
      "Epoch 422/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0557 - accuracy: 0.9828 - val_loss: 0.0451 - val_accuracy: 0.9869\n",
      "Epoch 423/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9826 - val_loss: 0.0421 - val_accuracy: 0.9892\n",
      "Epoch 424/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9805 - val_loss: 0.0467 - val_accuracy: 0.9892\n",
      "Epoch 425/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0571 - accuracy: 0.9836 - val_loss: 0.0498 - val_accuracy: 0.9854\n",
      "Epoch 426/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0527 - accuracy: 0.9854 - val_loss: 0.0419 - val_accuracy: 0.9885\n",
      "Epoch 427/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9854 - val_loss: 0.0414 - val_accuracy: 0.9877\n",
      "Epoch 428/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0522 - accuracy: 0.9854 - val_loss: 0.0413 - val_accuracy: 0.9862\n",
      "Epoch 429/2000\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9823 - val_loss: 0.0432 - val_accuracy: 0.9846\n"
     ]
    }
   ],
   "source": [
    "# 학습이 언제 자동 중단 될지를 설정합니다.\n",
    "# 20번 이상 더이상 오차가 작아지지 않으면 학습을 멈춰라\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#최적화 모델이 저장될 폴더와 모델의 이름을 정합니다.\n",
    "modelpath=\"./data/model/Ch14-4-bestmodel.hdf5\"\n",
    "\n",
    "# 최적화 모델을 업데이트하고 저장합니다.\n",
    "# 마지막 모델 하나만 저장해라\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "#모델을 실행합니다.\n",
    "history=model.fit(X_train, y_train, epochs=2000, batch_size=500, validation_split=0.25, verbose=1, callbacks=[early_stopping_callback,checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 815us/step - loss: 0.0601 - accuracy: 0.9792\n",
      "Test accuracy: 0.9792307615280151\n"
     ]
    }
   ],
   "source": [
    "# 테스트 결과를 출력합니다.\n",
    "score=model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
